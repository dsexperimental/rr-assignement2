---
title: "Human and Economic Costs by Storm Type"
author: "Dave Sutter"
date: "2022-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(lubridate)
library(stringr)
library(jsonlite)
```

# Synopsis

We examine the storm data file provided by the United States National Weather 
Service to find the storm types that do the most total yearly damage in terms
of fatalities, injuries and dollar cost damage.

The storm data file includes storm events from 1950 until 2011 in the version 
of the data used. We limited the events to those between 1996, where the data
quality increased, and the last complete year of data, since the storm types
vary by time of year. 

The main task in analyzing the file is interpreting the storm type labels.
We used keyword matching, using a curated list of keywords to help map the event 
type labels to the official storm type names used by the National 
Weather Service.

We then summed the total fatalities, injuries and damage costs for each type and 
averaged to estimate the yearly amounts.

The storm type with the highest average yearly fatalities is _Excessive Heat_, 
injuries is _Tornado_, and dollar damage is _Flood_.

# Data Processing

We are processing storm data from the United States National Weather Service.

### Source Data Used in Analysis

National Weather Service Storm Data, 1950 to Nov, 2011:

https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2

National Weather Service Storm Data Documentation:

https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf

National Climatic Data Center Storm Events FAQ:

https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf

There is updated storm data available from the National Weather Service website:

https://www.ncdc.noaa.gov/stormevents/details.jsp

### Utility Functions {#utilityFunctions}

The following are some simple functions that are used in the analysis. It is
recommended that you skip this section and come back to it later for more
information if desired

``` {r}
##=====================================
## This function converts exponent strings to numerical exponents
##=====================================
## This converts exponent string values that are
## - a number of metric exponents - see code for selection
## - numeric strings (convert to numeric)
## - other - anything else is converted to an exponent of 0
## In addition a safety text is done that there are no single letter
## values that are not converted by the metric coefficients, with the
## assumption that theis coefficient should be added.
convertExpCol <- function(expStr) {
  expNum <- numeric(length(expStr))
  expNum <- NA
  
  expNum[str_detect(expStr,"[hH]")] = 2
  expNum[str_detect(expStr,"[kK]")] = 3
  expNum[str_detect(expStr,"[mM]")] = 6
  expNum[str_detect(expStr,"[bB]")] = 9
  expNum[str_detect(expStr,"[tT]")] = 12
  
  isnum <- str_detect(expStr,"[0-9]+")
  expNum[isnum] = as.numeric(expStr[isnum])
  
  ## SAFETY TEST --------
  ## Check for any remaining single letter exponents
  ## These are probably metric prefixes we left out
  singleLetterExpLeft <- sum(str_detect(expStr[is.na(expNum)],"[a-zA-Z]"))
  if(singleLetterExpLeft > 0) {
    error("There are unaccounted letter values for exponent! Investigate")
  }
  ##--------------
  
  expNum[is.na(expNum)] = 0 
  
  expNum
}

## standardize string - lowercase and replace dash with space
stdString <- function(textVector) {
  str_replace(tolower(textVector),"-"," ")
}


## this function takes a vector of outerStrings and innerStrings and returns a matrix
## telling if a given outerString contains a given innerString. The outerStrings represent
## row indices and the innerStrings represent column indices
getContainedIn <- function(outerStrings,innerStrings) {
  inner_in_outer <- matrix(FALSE,nrow = length(outerStrings),ncol = length(innerStrings))
  dimnames(inner_in_outer) <- list(outerStrings,innerStrings)
  
  setRow <- function(outerString) {
    inner_in_outer[outerString,] <<- str_detect(outerString,regex(innerStrings,ignore_case=TRUE))
  }
  
  sapply(outerStrings,setRow)
  
  inner_in_outer
}
```

### Loading Data

Here we read in the data from the NWS storm data file.

The storm file has been downloaded from the URL listed above and is present
as the location specified in the code below.

We added some additional code to specify the column classes and to exclude
some unneeded columns, to limit the resources needed at load time.

There is also a safety check to verify the format of the file is as expected.
An error will be given if the format of the file does not match the expected
format. If this is true, a number of names in the code will have to be updated
to run this document.

``` {r}

## This is a list of column names for the data
colNames <- c("STATE__", "BGN_DATE", "BGN_TIME", "TIME_ZONE", "COUNTY", 
              "COUNTYNAME", "STATE", "EVTYPE", "BGN_RANGE", "BGN_AZI",
              "BGN_LOCATI", "END_DATE", "END_TIME", "COUNTY_END", "COUNTYENDN", 
              "END_RANGE", "END_AZI", "END_LOCATI", "LENGTH", "WIDTH", "F",
              "MAG", "FATALITIES", "INJURIES", "PROPDMG", "PROPDMGEXP", 
              "CROPDMG", "CROPDMGEXP", "WFO", "STATEOFFIC", "ZONENAMES", 
              "LATITUDE", "LONGITUDE", "LATITUDE_E", "LONGITUDE_", "REMARKS",
              "REFNUM") 

## place the column names you want to keep here
colToKeep <- c("BGN_DATE","EVTYPE","FATALITIES","INJURIES","PROPDMG",
               "PROPDMGEXP","CROPDMG","CROPDMGEXP","REMARKS")

## place the column classes here, to help loading
colToKeepClasses <- c("character","character","numeric","numeric","numeric",
                      "character","numeric","character","character")

## generate the col class vector
colClasses <- rep("NULL",length(colNames))
colClasses[match(colToKeep,colNames)] <- colToKeepClasses

## load the csv file
dat <- read.csv("repdata_data_StormData.csv.bz2",colClasses = colClasses)

## SAFETY TEST: verify the format is as expected
if(!identical(colToKeep,names(dat))) {
  error("File not loaded properly! Format may have changed since origianl analysis")
}

```

Here we preprocess the storm data to get it in the desired form to start our analysis.

- Convert the date column to a date object. For the analysis we consider the 
time for the storm to be the begin date.
- Filter out data older than Jan 1, 1996. Before this time the data is less 
complete and less standardized.
- Filter out any partial year data corresponding to a partial year at the end
of the file. We want to consider only full years since different times of the
year will have a different concentration of storm types.
- We standardize the type label (EVTYPE column) to be lower case and we remove
any dashes, since these are not used reliably in the data.
- We convert the property damage exponent and drop damage exponent to a 
numerical value.
- We add a column of total damage, with units of dollars.

Rather than truncating any final partial year of data, we could instead impute 
values for events not included for the last year. We did not do this however 
because there seems to be adequate statistics without that partial year data.

``` {r}

## convert time from character class
dat$BGN_DATE <- mdy_hms(dat$BGN_DATE)

## we will only use data from 1996 and newer
dat <- filter(dat,BGN_DATE >= "1996-01-01")

## the analysis only examines data that contains damage, fatalities or injuries
dat <- filter(dat,((PROPDMG > 0) | (CROPDMG > 0) | (FATALITIES > 0) | (INJURIES > 0)) )

## standardize the format of the type labels
dat$EVTYPE <- stdString(dat$EVTYPE)

## add a column for damage amounts
## first convert character "exponent" columns to numerical values
dat$PEXP <- convertExpCol(dat$PROPDMGEXP)
dat$CEXP <- convertExpCol(dat$CROPDMGEXP)
dat$DMG <- dat$PROPDMG * (10^dat$PEXP) + dat$CROPDMG * (10^dat$CEXP)

## check if we want to truncate data at the end of the file for being a partial year
## we will call (using the julian day of the year)
PARTIAL_YEAR_CUTOFF <- 350
maxDate <- max(dat$BGN_DATE)

if(yday(maxDate) < PARTIAL_YEAR_CUTOFF) {
  partialYear = year(maxDate)
  dat <- filter(dat,year(BGN_DATE) < partialYear)
}

```

### Data For Label Analysis

Here we load the data we will use to identify the event type from the even label.

In the analysis we use the official storm types from the National Weather 
Service. The values were obtained from the latest version of the Storm Data
documentation, July 26, 2021, obtained at the URL below.

https://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf

Throughout the analysis, we use modified version of the string names. All 
letters are lowercase and the _dash_ character is removed. This is done in the
function _stdString()_ from the section [Utility Functions](#utilityFunctions).

``` {r}
## official types from NWS
officialTypesJson <- '["Tsunami", "Storm Surge/Tide", "High Surf", "Seiche", 
                "Rip Current", "Astronomical Low Tide", "Flash Flood", "Flood", 
                "Coastal Flood", "Lakeshore Flood", "Heavy Rain", "Hail", 
                "Lightning", "Hurricane (Typhoon)", "Tropical Depression", 
                "Tropical Storm", "Thunderstorm Wind", "High Wind", "Strong Wind", 
                "Dust Storm", "Tornado", "Funnel Cloud", "Waterspout", 
                "Dust Devil", "Blizzard", "Winter Storm", "Heavy Snow", 
                "Ice Storm", "Lake-Effect Snow", "Sleet", "Winter Weather", 
                "Avalanche", "Extreme Cold/Wind Chill", "Frost/Freeze", 
                "Cold/Wind Chill", "Excessive Heat", "Heat", "Dense Fog", 
                "Freezing Fog", "Debris Flow", "Wildfire", "Dense Smoke", 
                "Drought", "Volcanic Ash", "Marine Hail", "Marine High Wind", 
                "Marine Strong Wind", "Marine Thunderstorm Wind"]'

officialTypes <- fromJSON(officialTypesJson)

## convert the official types to the working format: lowercase, "-" removed
types <- stdString(officialTypes)
```

An important part of our analysis is to identify the storm type based on the
labels from the storm data. To help doing this we use the official type names
above but we also consider several aliases for these names. 

The _typeAliases_ are a mapping with the key being the type name and the value
being an array of alias values.

The aliases to use were determined by iteratively completing the analysis and
examining the values that were matched. We entered this data in the form
of a json. It can be copied into a file and loaded from there rather than
from the text entered below. This allows you to experiment with your own
alias values.

After reading in the type alias data, we convert into a mapping where the key is
the alias and the value is the type name, since this is the format we will use
in our later analysis.

Finally, we also include a plain vector of the alias values.

In the document section [Supplemental Data](#supplementalData), we have some discussion of the
alias values that were included and omitted.

``` {r}

## type aliases
## - list names: NWS types
## - list values: aliases for these types 
typeAliasesJson <- '{
  "cold/wind chill": ["cold", "wind chill", "windchill", "hypothermia/exposure", "hyperthermia/exposure"],
  "debris flow": ["landslide", "rock slide", "mud slide", "mudslide", "landslump"],
  "dense fog": ["fog"],
  "dust storm": ["blowing dust"],
  "extreme cold/wind chill": ["extreme cold", "extreme wind chill", "extreme windchill"],
  "freezing fog": ["glaze"],
  "frost/freeze": ["frost", "freeze", "black ice"],
  "flood": ["urban/sml stream fld", "dam break"],
  "flash flood": ["flash flood/flood", "flood/flash/flood"],
  "heat": ["unseasonably warm", "warm weather"],
  "heavy rain": ["torrential rainfall", "hvy rain", "rain", "unseasonal rain"],
  "heavy snow": ["excessive snow"],
  "high surf": ["heavy surf", "rough surf", "hazardous surf", "heavy rain/high surf", "heavy surf and wind", "heavy surf/high surf"],
  "high wind": ["non tstm wind"],
  "hurricane (typhoon)": ["hurricane", "typhoon", "hurricane/typhoon"],
  "storm surge/tide": ["storm surge"],
  "strong wind": ["gusty wind", "gradient wind", "wind damage", "wind","gusty wind/rain","gusty wind/hvy rain"],
  "thunderstorm wind": ["tstm wind", "tstmw", "gustnado", "downburst", "microburst", "tstm wind/hail", "gusty wind/hail", "tstm wind and lightning"],
  "tornado": ["landspout"],
  "wildfire": ["wild fire", "grass fire", "forest fire", "brush fire"],
  "winter weather": ["freezing rain", "light snowfall", "snow and ice", "late season snow", "light snow", "snow", "freezing drizzle", "rain/snow", "snow squall", "mixed precipitation", "blowing snow", "wintry mix", "mixed precip", "snow squalls", "falling snow/ice", "cold and snow", "ice roads", "icy roads", "ice on roads", "ice on road"]
}'
typeAliases <- fromJSON(typeAliasesJson)

## invert mapping to go from aliases to types
aliasToType <- list()
addTypeAlias <- function(type,aliases) {
  aliasToType[aliases] <<- type
}
dummy <- mapply(addTypeAlias,names(typeAliases),typeAliases,SIMPLIFY=FALSE)

##this is a vector with the aliases in it
aliases <- names(aliasToType)

```

For the analysis, we construct a _keywords_ vector. This includes the proper 
type names and the aliases. 

Along with this, we create a mapping where the key is the keyword (type names
and aliases) and the value is the associated type name. 

``` {r}
## keywords - a list of types and aliases we will use to identify type from labels
keywords <- c(types,aliases)

##create mapping from keywords to types, to identify types from matched keywords
typeToType <- as.list(types)
names(typeToType) <- types
keywordToType <- c(typeToType,aliasToType)
```

### Identify Type from Event Labels

Here we conduct an analysis to convert the labels on the storm data file into
proper type names.

We first find all keywords that are contained in each label. (Recall keywords
are composed of the proper type names and the type name aliases). 

There are keywords that contain other keywords, such as _flash flood_ and _flood_.
For cases like this, we only want to match the longer of the keyword, and not
the keyword it contains. To account for this we remove any keyword _A_ from
the list of keywords contained in the given label if a keyword _B_ is also 
contained in that label and that keyword contains the keyword _A_.

Once we have the keyword matches for a given label, we assign a type to the 
label as follows:
- If there is only one matching keyword, this determines the type
- If there are no matching keywords, set the type as the label enclosed in 
double arrows, _<<_ and _>>_. 
- If there are multiple matching keywords, give a warning and set the type
as the label enclosed in _<<_ and _-n>>_ where n is the number of matching
keywords.

Thus, we only assign a proper type in the case where a unique keyword is found for
the label. Otherwise we assign an "improper" type, by embedding the label in
arrow characters. We keep these values through our analysis so we can see if
they are significant.

For labels that do not get a proper type, if desired we can add 
a new entry to the alias file to give a desired type mapping.

One particular case of this we address is legitimate multiple matches, such as 
"gusty wind/hail". By adding the alias we can explicitly assign it to a single
selected type. An aid to doing this is to read the "REMARK" field to get an 
idea of the actual reported damage from the storms.

An alternate approach to handling these legitimate multiple matches would 
be to fractionally map the event to each type, such as assigning a fractional 
damage to each type based on the relative total damage done by those types.


``` {r}

##vector of labels from data set
labels <- unique(dat$EVTYPE)

## k_in_l: logical matrix with an entry TRUE telling if a key (column index) is
## in a label (row index)
k_in_l <- getContainedIn(labels,keywords)

## k_in_k: logical matrix with an entry TRUE telling if a keyword (column index) is
## contained in a keyword (row index)
k_in_k <- getContainedIn(keywords,keywords)

## adj_k_in_k: this is the same as k_in_k except we remove the diagonal entries
## which say a key contains itself
adj_k_in_K <- k_in_k
diag(adj_k_in_K) <- FALSE

## if a longer key contains a shorter key, any label that contains a longer
## key will also contain the shorter key. We want to discard the shorter key
## in this case. The equation below does this. Explanation:
## - k_in_l[label,long key] * adj_k_in_k[long key,short key]: This is true if
##   both a ("long") key contains a ("short") key and a label contains the 
##   ("long") key.
## - (k_in_l %*% adj_k_in_k): for a given label and key, this is true if any
##   other keys both contain this key and are contained in this label
## - k_in_l & !as.logical(k_in_l %*% adj_k_in_K): For any long key that is
##   contained in a label, this clears the flag that states on "short" key contained
##   in this "long" key is also contained in the label
fixed_k_in_l <- k_in_l & !as.logical(k_in_l %*% adj_k_in_K)


##get the list of keywords in each label
getKeywords <- function(innerKeyFlags) {
  keywords[innerKeyFlags]
}
labelKeywords <- apply(fixed_k_in_l,1,getKeywords)

## Here we make a map from the labels to the type
## - if the label contains one keyword (type or alias), we associate it with that type
## - if the label contains multiple keywords (type or alias), we record the type
## as the value of the label in the format <<label-n>> where n is the number of
## keywords matched. We also display a warning. The intention is that multiple
## matches should be explicitly mapped to a type using aliases.
## - it the label contains 0 or multiple keywords,  we record the type
## as the value of the label in the format <<label>>, to show a failed match
getKeywordTypes <- function(keywords,label) {
  if(length(keywords) == 1) {
    type <- keywordToType[[keywords[1]]]
  }
  else if(length(keywords) > 1) {
    warning(sprintf("Label matching multiple keywords found: %s: %S",label,as.string(keywords)))
    type <- sprintf("<<%s-%i>>",label,length(keywords))
  }
  else {
    type <- sprintf("<<%s>>",label)
  }
  type
}
labelToType <- mapply(getKeywordTypes,labelKeywords,label=names(labelKeywords),SIMPLIFY=FALSE)


## we use our label to Type mapping to add a "type" column to our data.
## this will either include a valid type name or the label enclosed in "<<" + ">>"
dat$type <- factor(unlist(labelToType[dat$EVTYPE]))
```

### Summing the Storm Damage

Here we calculate the yearly average costs for all storms over the period of
the data.

``` {r}
##===============================
## Get the total costs, fatalities and injuries
##===============================

## get total values
totalCost <- sum(dat$DMG)
totalFatalities <- sum(dat$FATALITIES)
totalInjuries <- sum(dat$INJURIES)

## get the number of years
numberYears <- year(max(dat$BGN_DATE)) - year(min(dat$BGN_DATE)) + 1

## get the yearly average values
aveYearlyCost <- totalCost / numberYears
aveYearlyFatalities <- totalFatalities / numberYears
aveYearlyInjuries <- totalInjuries / numberYears
```

Here we create a data frame each for fatalities, injuries and damage cost in 
dollars. Each one contains the average yearly value for each type, ordered from
maximum to minimum, along with the average yearly number of events.

Keep in mind we have previously filtered data to only consider events that have
as least some fatalities, injuries or damage.

``` {r}

## group the data by type
groupedDat <- dat %>% group_by(type)

## for each category (fatalities, injuries and damage), create a ordered
## data frame of totals by type

fatalitiesDat <- groupedDat %>% 
  summarise(fatalities = sum(FATALITIES) / numberYears, count = length(FATALITIES) / numberYears) %>% 
  arrange(desc(fatalities))

injuriesDat <- groupedDat %>% 
  summarise(injuries = sum(INJURIES) / numberYears, count = length(INJURIES) / numberYears) %>% 
  arrange(desc(injuries))

costDat <- groupedDat %>% 
  summarise(cost = sum(DMG) / numberYears, count = length(DMG) / numberYears) %>% 
  arrange(desc(cost))
```


### Preprocessing of Results Data for Plots

Here we do some preprocessing of the data to prepare it for plotting.

We will present the data in a pie chart so we want to group any types with
small values into a single _other_ category.

In this code block, we also define a color palette we will use with the pie
charts.

``` {r}
##=================================
## Preprocessing for plotting
##=================================

## this function takes a data frame with three columns
## type (factor), value, count
## it removes all rows that are smaller than a given fraction and replaces these
## with an "other" row
## NOTE: there should not be an existing category named "other"
addOtherCat <- function(fullDF,cutoffFrac) {
  #truncate the data frame
  totalValue <- sum(fullDF[[2]])
  totalCount <- sum(fullDF[[3]])
  redDF <- filter(fullDF,fullDF[[2]] / totalValue >= cutOffFrac)
  
  ##get the "other" value and count and append it
  otherValue <- totalValue - sum(redDF[[2]])
  otherCount <- totalCount - sum(redDF[[3]])
  levels(redDF[[1]]) <- c(levels(redDF[[1]]),"other")
  newRow <- list("other",otherValue,otherCount)
  names(newRow) <- names(redDF)
  redDF[length(redDF[[1]]) + 1,] <- newRow
  redDF
}

##this is the fractional value below which we wil place in the "other" type
cutOffFrac <- 0.007

# create modified data frames keeping only the major types
## which are data readable from the pie chart
redCostDat <- addOtherCat(costDat,cutOffFrac)
redFatalitiesDat <- addOtherCat(fatalitiesDat,cutOffFrac)
redInjuriesDat <- addOtherCat(injuriesDat,cutOffFrac)

## color palette
pal <- colorRampPalette(palette.colors(n=8,palette = "Pastel 2"))
otherColor <- rgb(230,230,230,maxColorValue=255)
```

# Results

### Fatalities

Here we show a plot of the storm types with the highest number of fatalities.

``` {r, fig.height=6,fig.width=7}
pie(redFatalitiesDat$fatalities,
    labels=sprintf("%s (%.0f)",redFatalitiesDat$type,redFatalitiesDat$fatalities),
    cex=0.6,init.angle=20,col=c(pal(length(redFatalitiesDat$type)-1),otherColor),
    main="Yearly Average Fatalities by Storm Type",
    sub = sprintf("Total Average Yearly Fatalities: %.0f",aveYearlyFatalities))
```

### Injuries

Here we show a plot of the storm types with the highest number of injuries.

``` {r, fig.height=6,fig.width=7}
pie(redInjuriesDat$injuries,
    labels=sprintf("%s (%.0f)",redInjuriesDat$type,redInjuriesDat$injuries),
    cex=0.6,init.angle=20,col=c(pal(length(redInjuriesDat$type)-1),otherColor),
    main="Yearly Average Injuries by Storm Type",
    sub = sprintf("Total Average Yearly Injuries: %.0f",aveYearlyInjuries))
```

### Damage Cost in Dollars

Here we show a plot of the storm types with the greatest cost of damge.

``` {r, fig.height=6,fig.width=7}
pie(redCostDat$cost,
    labels=sprintf("%s $%.1fB",redCostDat$type,redCostDat$cost/1000000000),
    cex=0.75,init.angle=20,col=c(pal(length(redCostDat$type)-1),otherColor),
    main="Yearly Average Damage Cost in Dollars by Storm Type",
    sub = sprintf("Total Average Yearly Cost: $%.1fB",aveYearlyCost/1000000000))
```

# Supplemental Data {#supplementalData}

In this section we present some additional commentary and information, including 
tables of the complete sets of the results.

### General Remarks on Classification and Counting

- The _REMARK_ field of the storm data file includes valuable information on the
individual events. From this, it is evident the included labels are not always
a good description of the actual event. It appears some of the type labels may
have been generated from an automated analysis of these remarks. For more 
accurate labels, it might be worthwhile reexamining the type labeld using the 
_REMARK_ field.
- In looking at the data, repeat events were found. We did not make an effort to 
identify these. This is hoever another area that could be examined to get more
accurate data.

### Yearly Average Fatalities by Storm Type

Here we present the complete table of yearly average fatalities by storm type.

``` {r}
## complete fatalities table
print(fatalitiesDat,n=length(fatalitiesDat$type))
```

### Yearly Average Injuries by Storm Type

Here we present the complete table of yearly average injuries by storm type.

``` {r}
## complete injuries table
print(injuriesDat,n=length(injuriesDat$type))
```

### Yearly Average Damage Costs by Storm Type

Here we present the complete table of yearly average damage costs in dollars by storm type.

``` {r}
## complete cost table
print(costDat,n=length(costDat$type))
```

### Keyword Matching Analysis

Here we present some of the event label matching results. This can be used when
to check the aliases that were used, and see if some new aliases should be added
or if some should be removed.

We also include some commentary on the aliases we chose to include and not
include in this section.

First we calculate a vector of keyword matching counts for each type label from
the storm data file.

``` {r}
## this is the count for a given label keyword
labelKeywordCounts <- sapply(labelKeywords,length)
```

##### Labels with no Keyword Matches

``` {r}
## labels with no keyword matches
names(labelKeywords[labelKeywordCounts == 0])
```

##### Labels with One Keyword Match

``` {r}
## labels with 1 keyword match
names(labelKeywords[labelKeywordCounts == 1])
```

##### Labels with Two or More Keyword Matches

As part of our analysis, we explicitly added aliases to map any event labels 
with multiple matching keywords to control how the event was mapped. 

As stated in the document, other approaches could have been taken to map data
from events labeled with multiple storm types.

``` {r}
## labels with 2 keyword matches
names(labelKeywords[labelKeywordCounts >= 2])
```
##### Notable Included Aliases

Here we list some of our aliases that involved a judgement, as opposed to being
alternate terms for a type.

Here we reference the storm types document, which can be obtained from
https://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf.

- _rain_, _unseasonal rain_  => _heavy rain_: It is unclear that rain should be qualified as heavy
rain. The types description does not specify this. However we included it, 
given the inexact usage of the type label.
- _urban/sml stream fld_ => _flood_: This actually specified in the types document.
In the latest version (2021) it specifies this should be classified as a _flood_,
whereas some earlier versions specified this should go in the category of _heavy rain_.
- _blowing dust_ => _dust storm_: It is unclear if blowing dust rises to the level
to be called a storm, but as with rain vs heavy rain, we included it.
- _ice roads_, _icy roads_, _ice on roads_, _ice on road_ - We assume that generically
ice on roads was the result of precipitation, though it could be from other causes.
- _glaze_ => _freezing fog_: Glaze typically forms from freezing fog, or a similar
atmospheric effect.
- _black ice_ => _frost/freeze_: - Black ice can form from moister on the ground
or condensing from the air similar to glaze. We decided to map this to frost/freeze.
- _dam break_ => _flood_: In the instances of this label the dam break led to 
flooding, and was cause, at least partially, by flooding.
- _coastal erosion_, _beach erosion_ - The erosion is typiclly casued by high surf.
- _cold and snow_ => _winter weather_ - Cold and snowis technically two of the storm
types but we labeled it as _winter weather_, the mildest form of snow, because
cold is assumed to go with snow.

The following are aliases the involve multiple storm types. In all these examples
a judgement was made on the apparent predominant damage type from reading the
_REMARK_ field of the storm data file. These are however cases where an alternate
strategy could be justified, such as count a fraction of the damage on each included type.

_gusty wind/rain_ => _strong wind_
_gusty wind/hvy rain_ => _strong wind_
_tstm wind/hail_ => _thunderstorm wind_
_gusty wind/hail_ => _thunderstorm wind_
_tstm wind and lightning_ => _thunderstorm wind_

##### Notable Omitted labels

- _thunderstorm_ - There was one event with this label, which actually corresponded
to a snow storm.
- _coastal storm_, _coastalstorm_ - Generically this could be many types of storms.
In most but not all of the few cases here it corresponded to a winter storm.
- _high water_ - This is an unclear term generically. In the examples included it
corresponded to flooding of streams
- _freezing spray_ - There is no proper category for this
- _rough seas_, _high swells_, _heavy seas_, _high seas_, _rogue wave_ - There is no 
category for this
- _marine accident_ - Generically it is unclear of the cause for this.
- _drowning_ - Generically it is unclear of the cause for this.
- _astronomical high tide_ - There is no proper category for this
- _other_ - Generically it is unclear of the cause for this. 

### Type Data

Below we print the official storm types and the type aliases used in the 
analysis.

##### NWS Storm Types

``` {r}

##NWS types, as listed from NWS
print(officialTypes)

```

##### Storm Type Aliases

``` {r}
print(typeAliases)
```
