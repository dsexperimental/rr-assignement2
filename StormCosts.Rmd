---
title: "Human and Economic Costs by Storm Type"
author: "Dave Sutter"
date: "2022-11-16"
output: html_document
---

# NOTES
 - Add the text!
 - I currently show my results for the complete set of data. I should give an
 average per year.
 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(lubridate)
library(stringr)
library(jsonlite)
```

# Synopsis

We examine the storm data file provided by the United States National Weather 
Service to find the storm types that do the most total yearly damage in terms
of fatalities, injuries and dollar damage.

The storm data file includes storm events from 1950 until 2011, in the version 
of the data used. We limited the data used to 1996 and later since the data 
after that point is more complete and is more standardized. 

Even for this range, the type labeling for the data is inconsistent. The main 
task in analyzing the file is interpreting the storm type. We used an 
analysis to map the event type labels in the data set to the official storm
type names used by the National Weather Service.

We then summed the total fatalities, injuries and damage costs for each type and 
averaged to estimate the yearly amounts.

The storm type with the highest average yearly fatalities is _Excessive Heat_, 
injuries is _Tornado_, and dollar damage is _Flood_.

# Data Processing

We are processing storm data from the United States National Weather Service.

## Source Data Used in Analysis

National Weather Service Storm Data, 1950 to Nov, 2011:

https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2

National Weather Service Storm Data Documentation:

https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf

National Climatic Data Center Storm Events FAQ:

https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf

There is updated storm data available from the National Weather Service website:

https://www.ncdc.noaa.gov/stormevents/details.jsp

## Utility Functions

The following are some simple functions that are used in the analysis. It is
recommended that you skip this section and come back to it later for more
information is desired

``` {r}
##=====================================
## This function converts exponent strings to numerical exponents
##=====================================
## This converts exponent string values that are
## - a number of metric exponents - see code for selection
## - numeric strings (convert to numeric)
## - other - anything else is converted to an exponent of 0
## In addition a safety text is done that there are no single letter
## values that are not converted by the metric coefficients, with the
## assumption that theis coefficient should be added.
convertExpCol <- function(expStr) {
  expNum <- numeric(length(expStr))
  expNum <- NA
  
  expNum[str_detect(expStr,"[hH]")] = 2
  expNum[str_detect(expStr,"[kK]")] = 3
  expNum[str_detect(expStr,"[mM]")] = 6
  expNum[str_detect(expStr,"[bB]")] = 9
  expNum[str_detect(expStr,"[tT]")] = 12
  
  isnum <- str_detect(expStr,"[0-9]+")
  expNum[isnum] = as.numeric(expStr[isnum])
  
  ## SAFETY TEST --------
  ## Check for any remaining single letter exponents
  ## These are probably metric prefixes we left out
  singleLetterExpLeft <- sum(str_detect(expStr[is.na(expNum)],"[a-zA-Z]"))
  if(singleLetterExpLeft > 0) {
    error("There are unaccounted letter values for exponent! Investigate")
  }
  ##--------------
  
  expNum[is.na(expNum)] = 0 
  
  expNum
}

## standardize string - lowercase and replace dash with space
stdString <- function(textVector) {
  str_replace(tolower(textVector),"-"," ")
}


## this function takes a vector of outerStrings and innerStrings and returns a matrix
## telling if a given outerString contains a given innerString. The outerStrings represent
## row indices and the innerStrings represent column indices
getContainedIn <- function(outerStrings,innerStrings) {
  inner_in_outer <- matrix(FALSE,nrow = length(outerStrings),ncol = length(innerStrings))
  dimnames(inner_in_outer) <- list(outerStrings,innerStrings)
  
  setRow <- function(outerString) {
    inner_in_outer[outerString,] <<- str_detect(outerString,regex(innerStrings,ignore_case=TRUE))
  }
  
  sapply(outerStrings,setRow)
  
  inner_in_outer
}
```

## Loading Data

Here we read in the data from the NWS storm data file.

The storm file has been downloaded from the URL listed above and is present
as the location specified in the code below.

We added some additional code to specify the column classes and to exclude
some unneeded columns, to limit the resources needed at load time.

There is also a safety check to verify the format of the file is as expected.
An error will be given if the format of the file does not match the expected
format. If this is true, a number of names in the code will have to be udpated
to run this document.

``` {r}

## This is a list of column names for the data
colNames <- c("STATE__", "BGN_DATE", "BGN_TIME", "TIME_ZONE", "COUNTY", 
              "COUNTYNAME", "STATE", "EVTYPE", "BGN_RANGE", "BGN_AZI",
              "BGN_LOCATI", "END_DATE", "END_TIME", "COUNTY_END", "COUNTYENDN", 
              "END_RANGE", "END_AZI", "END_LOCATI", "LENGTH", "WIDTH", "F",
              "MAG", "FATALITIES", "INJURIES", "PROPDMG", "PROPDMGEXP", 
              "CROPDMG", "CROPDMGEXP", "WFO", "STATEOFFIC", "ZONENAMES", 
              "LATITUDE", "LONGITUDE", "LATITUDE_E", "LONGITUDE_", "REMARKS",
              "REFNUM") 

## place the column names you want to keep here
colToKeep <- c("BGN_DATE","EVTYPE","FATALITIES","INJURIES","PROPDMG",
               "PROPDMGEXP","CROPDMG","CROPDMGEXP","REMARKS")

## place the column classes here, to help loading
colToKeepClasses <- c("character","character","numeric","numeric","numeric",
                      "character","numeric","character","character")

## generate the col class vector
colClasses <- rep("NULL",length(colNames))
colClasses[match(colToKeep,colNames)] <- colToKeepClasses

## load the csv file
dat <- read.csv("repdata_data_StormData.csv.bz2",colClasses = colClasses)

## SAFETY TEST: verify the format is as expected
if(!identical(colToKeep,names(dat))) {
  error("File not loaded properly! Format may have changed since origianl analysis")
}

```

Here we preprocess the storm data to get it in the desired form to start our analysis.

- Convert the date column to a date object. For the analysis we consider the 
time for the storm to be the begin date.
- Filter out data older than Jan 1, 1996. Before this time the data is less 
complete and less standardized.
- Filter out any partial year data corresponding to a partial year at the end
of the file. We want to consider only full years since different times of the
year will have a different concentration of storm types.
- We standardize the type label (EVTYPE column) to be lower case and we remove
any dashes, since these are not used reliably in the data.
- We convert the property damage exponent and drop damage exponent to a 
numerical value.
- We add a column of total damage, with units of dollars.

Rather than truncating any final partial year of data, we could instead impute 
values for events not included for the last year. We did not do this however 
because there seems to be adequate statistics without that partial year data.

``` {r}

## convert time from character class
dat$BGN_DATE <- mdy_hms(dat$BGN_DATE)

## we will only use data from 1996 and newer
dat <- filter(dat,BGN_DATE >= "1996-01-01")

## the analysis only examines data that contains damage, fatalities or injuries
dat <- filter(dat,((PROPDMG > 0) | (CROPDMG > 0) | (FATALITIES > 0) | (INJURIES > 0)) )

## standardize the format of the type labels
dat$EVTYPE <- stdString(dat$EVTYPE)

## add a column for damage amounts
## first convert character "exponent" columns to numerical values
dat$PEXP <- convertExpCol(dat$PROPDMGEXP)
dat$CEXP <- convertExpCol(dat$CROPDMGEXP)
dat$DMG <- dat$PROPDMG * (10^dat$PEXP) + dat$CROPDMG * (10^dat$CEXP)

## check if we want to truncate data at the end of the file for being a partial year
## we will call (using the julian day of the year)
PARTIAL_YEAR_CUTOFF <- 350
maxDate <- max(dat$BGN_DATE)

if(yday(maxDate) < PARTIAL_YEAR_CUTOFF) {
  partialYear = year(maxDate)
  dat <- filter(dat,year(BGN_DATE) < partialYear)
}

```

In the analysis we use the official storm types from the National Weather 
Service. The values were obtained from the latest version of the Storm Data
documentation, July 26, 2021, obtained at the URL below.

https://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf

Throughout the analysis, we use modified version of the string names. All 
letters are lowercase and the _dash_ character is removed. This is done in the
function stdString() from the _Utility Function_ section above.

``` {r}
## official types from NWS
officialTypesJson <- '["Tsunami", "Storm Surge/Tide", "High Surf", "Seiche", 
                "Rip Current", "Astronomical Low Tide", "Flash Flood", "Flood", 
                "Coastal Flood", "Lakeshore Flood", "Heavy Rain", "Hail", 
                "Lightning", "Hurricane (Typhoon)", "Tropical Depression", 
                "Tropical Storm", "Thunderstorm Wind", "High Wind", "Strong Wind", 
                "Dust Storm", "Tornado", "Funnel Cloud", "Waterspout", 
                "Dust Devil", "Blizzard", "Winter Storm", "Heavy Snow", 
                "Ice Storm", "Lake-Effect Snow", "Sleet", "Winter Weather", 
                "Avalanche", "Extreme Cold/Wind Chill", "Frost/Freeze", 
                "Cold/Wind Chill", "Excessive Heat", "Heat", "Dense Fog", 
                "Freezing Fog", "Debris Flow", "Wildfire", "Dense Smoke", 
                "Drought", "Volcanic Ash", "Marine Hail", "Marine High Wind", 
                "Marine Strong Wind", "Marine Thunderstorm Wind"]'

officialTypes <- fromJSON(officialTypesJson)

## convert the official types to the working format: lowercase, "-" removed
types <- stdString(officialTypes)
```

An important part of our analysis is to identify the storm type based on the
labels from the storm data. To help doing this we use the official type names
above but we also consider several aliases for these names. 

The typeAliases are a mapping with the key being the type name and the value
being an array of alias values.

The aliases to use were determined by iteratively completing the analysis and
seeing examining the values that were matched. We entered this data in the form
of a json. It can be copied into a file and loaded from there rather than
from the text entered below. This allows you to experiment with your own
alias values.

After reading in the typeAlias data, we convert into a mapping where the key is
the alias and the value is the type name, since this is the format we will use
in our later analysis.

Finally, we also include a plain vector of the alias values.

``` {r}

## type aliases
## - list names: NWS types
## - list values: aliases for these types 
typeAliasJson <- '{
  "cold/wind chill": ["cold", "wind chill", "windchill", "hypothermia/exposure", "hyperthermia/exposure"],
  "debris flow": ["landslide", "rock slide", "mud slide", "mudslide", "landslump"],
  "dense fog": ["fog"],
  "dust storm": ["blowing dust"],
  "extreme cold/wind chill": ["extreme cold", "extreme wind chill", "extreme windchill"],
  "freezing fog": ["glaze"],
  "frost/freeze": ["frost", "freeze", "black ice"],
  "flood": ["dam break"],
  "flash flood": ["flash flood/flood", "flood/flash/flood"],
  "heat": ["unseasonably warm", "warm weather"],
  "heavy rain": ["torrential rainfall", "urban/sml stream fld", "hvy rain", "rain", "unseasonal rain"],
  "heavy snow": ["excessive snow"],
  "high surf": ["heavy surf", "rough surf", "hazardous surf"],
  "high wind": ["non tstm wind", "non-tstm wind"],
  "hurricane (typhoon)": ["hurricane", "typhoon", "hurricane/typhoon"],
  "lake-effect snow": ["lake effect snow"],
  "storm surge/tide": ["storm surge"],
  "strong wind": ["gusty wind", "gradient wind", "wind damage", "wind"],
  "thunderstorm wind": ["tstm wind", "tstmw", "gustnado", "downburst", "microburst"],
  "tornado": ["landspout"],
  "wildfire": ["wild fire", "grass fire", "forest fire", "brush fire"],
  "winter weather": ["freezing rain", "light snowfall", "snow and ice", "late season snow", "light snow", "snow", "freezing drizzle", "rain/snow", "snow squall", "mixed precipitation", "blowing snow", "wintry mix", "mixed precip", "snow squalls", "falling snow/ice", "cold and snow", "ice roads", "icy roads", "ice on roads"]
}'
typeAliases <- fromJSON(typeAliasesJson)

## invert mapping to go from aliases to types
aliasToType <- list()
addTypeAlias <- function(type,aliases) {
  aliasToType[aliases] <<- type
}
dummy <- mapply(addTypeAlias,names(typeAliases),typeAliases,SIMPLIFY=FALSE)

##this is a vector with the aliases in it
aliases <- names(aliasToType)

```

For the analysis, we construct a _keywords_ vector. This includes the proper 
type names and the alias name. 

Along with this, we create a mapping where the key is the keyword (type names
and aliases) and the value is the associated type name. 

``` {r}
## keywords - a list of types and aliases we will use to identify type from labels
keywords <- c(types,aliases)

##create mapping from keywords to types, to identify types from matched keywords
typeToType <- as.list(types)
names(typeToType) <- types
keywordToType <- c(typeToType,aliasToType)
```

## Cleaning the Storm Type Labels

Here we conduct an analysis to convert the labels on the storm data file into
proper type names.

We first find all keywords that are contained in each label. (Recall a keywords
are composed of the proper type names and the type name aliases). 

There are keywords that contain other keywords, such as _flash flood_ and _flood_.
For cases like this, we only want to match the longer of the keyword, and not
the keyword it contains. To account for this we remove any keyword _A_ from
the list of keywords contained in the given label if a keyword _B_ is also 
contained in the label and that keyword contains the keyword _A_.

Once we have the keyword matches for a given label, we assign a type to the 
label as follows:
- If there is only one matching keyword, this determines the type
- If there are multiple matching keywords, we take the first one __DOH!__ 
- If there are no matching keywords, we create a keyword by enclosing 
the label in double arrows, _<<_ and _>>_. We carry this data through
our calculation and we can see if any of these form a significant portion of our
data.

# DOH! WE TAKE THE FIRST ONE BASED ON OUR ARBITRARY ORDERING!!! FIX THAT!!!

``` {r}

##vector of labels from data set
labels <- unique(dat$EVTYPE)

## k_in_l: logical matrix with an entry TRUE telling if a key (column index) is
## in a label (row index)
k_in_l <- getContainedIn(labels,keywords)

## k_in_k: logical matrix with an entry TRUE telling if a keyword (column index) is
## contained in a keyword (row index)
k_in_k <- getContainedIn(keywords,keywords)

## adj_k_in_k: this is the same as k_in_k except we remove the diagonal entries
## which say a key contains itself
adj_k_in_K <- k_in_k
diag(adj_k_in_K) <- FALSE

## if a longer key contains a shorter key, any label that contains a longer
## key will also contain the shorter key. We want to discard the shorter key
## in this case. The equation below does this. Explanation:
## - k_in_l[label,long key] * adj_k_in_k[long key,short key]: This is true if
##   both a ("long") key contains a ("short") key and a label contains the 
##   ("long") key.
## - (k_in_l %*% adj_k_in_k): for a given label and key, this is true if any
##   other keys both contain this key and are contained in this label
## - k_in_l & !as.logical(k_in_l %*% adj_k_in_K): For any long key that is
##   contained in a label, this clears the flag that states on "short" key contained
##   in this "long" key is also contained in the label
fixed_k_in_l <- k_in_l & !as.logical(k_in_l %*% adj_k_in_K)


##get the list of keywords in each label
getKeywords <- function(innerKeyFlags) {
  keywords[innerKeyFlags]
}
labelKeywords <- apply(fixed_k_in_l,1,getKeywords)

## Here we make a map from the labels to the type
## - if the label contains one keyword (type or alias), we associate it with that type
## - if the label contains multiple keywords (type or alias), we associate it with
##   the fist keyword, with the idea the more damaging factor may likely be placed first. 
##   (alternately other weightings could be attemted)
## - it the label contains 0 or multiple keywords, we record the type as the 
## value of the label enclosed in double arrows, to flag a failed match.
getKeywordTypes <- function(keywords,label) {
  if(length(keywords) > 0) {
    type <- keywordToType[[keywords[1]]]
  }
  else {
    type <- sprintf("<<%s>>",label)
  }
  type
}
labelToType <- mapply(getKeywordTypes,labelKeywords,label=names(labelKeywords),SIMPLIFY=FALSE)


## we use our label to Type mapping to add a "type" column to our data.
## this will either include a valid type name or the label enclosed in "<<" + ">>"
dat$type <- factor(unlist(labelToType[dat$EVTYPE]))
```

## Summing the Storm Damage

Here we calculate the yearly average costs for all storms over the period of
the data.

``` {r}
##===============================
## Get the total costs, fatalities and injuries
##===============================

## get total values
totalCost <- sum(dat$DMG)
totalFatalities <- sum(dat$FATALITIES)
totalInjuries <- sum(dat$INJURIES)

## get the number of years
numberYears <- year(max(dat$BGN_DATE)) - year(min(dat$BGN_DATE)) + 1

## get the yearly average values
aveYearlyCost <- totalCost / numberYears
aveYearlyFatalities <- totalFatalities / numberYears
aveYearlyInjuries <- totalInjuries / numberYears
```

Here we create a data frame each for fatalities, injuries and damage cost in 
dollars. Each one contains the average yearly value for each type, ordered from
maximum to minimum, along with the average yearly number of events.

Keep in mind we have previously filtered data to only consider events that have
as least some fatalities, injuries or damage.

``` {r}

## group the data by type
groupedDat <- dat %>% group_by(type)

## for each category (fatalities, injuries and damage), create a ordered
## data frame of totals by type

fatalitiesDat <- groupedDat %>% 
  summarise(fatalities = sum(FATALITIES) / numberYears, count = length(FATALITIES) / numberYears) %>% 
  arrange(desc(fatalities))

injuriesDat <- groupedDat %>% 
  summarise(injuries = sum(INJURIES) / numberYears, count = length(INJURIES) / numberYears) %>% 
  arrange(desc(injuries))

costDat <- groupedDat %>% 
  summarise(cost = sum(DMG) / numberYears, count = length(DMG) / numberYears) %>% 
  arrange(desc(cost))
```


## Preprocessing of Results Data for Plots

Here we do some preprocessing of the data to prepare it for plotting.

We will present the data in a pie chart so we want to group any types with
small values into a single _other_ category.

In this code block, we also define a color palette we will use with the pie
charts.

``` {r}
##=================================
## Preprocessing for plotting
##=================================

## this function takes a data frame with three columns
## type (factor), value, count
## it removes all rows that are smaller than a given fraction and replaces these
## with an "other" row
## NOTE: there should not be an existing category named "other"
addOtherCat <- function(fullDF,cutoffFrac) {
  #truncate the data frame
  totalValue <- sum(fullDF[[2]])
  totalCount <- sum(fullDF[[3]])
  redDF <- filter(fullDF,fullDF[[2]] / totalValue >= cutOffFrac)
  
  ##get the "other" value and count and append it
  otherValue <- totalValue - sum(redDF[[2]])
  otherCount <- totalCount - sum(redDF[[3]])
  levels(redDF[[1]]) <- c(levels(redDF[[1]]),"other")
  newRow <- list("other",otherValue,otherCount)
  names(newRow) <- names(redDF)
  redDF[length(redDF[[1]]) + 1,] <- newRow
  redDF
}

##this is the fractional value below which we wil place in the "other" type
cutOffFrac <- 0.007

# create modified data frames keeping only the major types
## which are data readable from the pie chart
redCostDat <- addOtherCat(costDat,cutOffFrac)
redFatalitiesDat <- addOtherCat(fatalitiesDat,cutOffFrac)
redInjuriesDat <- addOtherCat(injuriesDat,cutOffFrac)

## color palette
pal <- colorRampPalette(palette.colors(n=8,palette = "Pastel 2"))
otherColor <- rgb(230,230,230,maxColorValue=255)
```

# Results

## Fatalities

Here we show a plot of the storm types with the highest number of fatalities.

``` {r, fig.height=6,fig.width=7}
pie(redFatalitiesDat$fatalities,
    labels=sprintf("%s (%.0d)",redFatalitiesDat$type,redFatalitiesDat$fatalities),
    cex=0.6,init.angle=20,col=c(pal(length(redFatalitiesDat$type)-1),otherColor),
    main="Fatalities by Storm Type",
    sub = sprintf("Total Fatalities: %.0d",totalFatalities))
```

## Injuries

Here we show a plot of the storm types with the highest number of injuries.

``` {r, fig.height=6,fig.width=7}
pie(redInjuriesDat$injuries,
    labels=sprintf("%s (%.0d)",redInjuriesDat$type,redInjuriesDat$injuries),
    cex=0.75,init.angle=20,col=c(pal(length(redInjuriesDat$type)-1),otherColor),
    main="Injuries by Storm Type",
    sub = sprintf("Total Injuries: %.0d",totalInjuries))
```

## Damage Cost in Dollars

Here we show a plot of the storm types with the greatest cost of damge.

``` {r, fig.height=6,fig.width=7}
pie(redCostDat$cost,
    labels=sprintf("%s $%.1fB",redCostDat$type,redCostDat$cost/1000000000),
    cex=0.75,init.angle=20,col=c(pal(length(redCostDat$type)-1),otherColor),
    main="Dollar Cost by Storm Type",
    sub = sprintf("Total Cost: $%.1fB",totalCost/1000000000))
```

# Supplemental Data

INTRO?

## Fatalities by Storm Type

Here we present the complete table of fatalities by storm type.

``` {r}
## complete fatalities table
print(fatalitiesDat,n=length(fatalitiesDat$type))
```

## Injuries by Storm Type

Here we present the complete table of injuries by storm type.

``` {r}
## complete injuries table
print(injuriesDat,n=length(injuriesDat$type))
```

## Damage Costs by Storm Type

Here we present the complete table of damage costs by storm type.

``` {r}
## complete cost table
print(costDat,n=length(costDat$type))
```

## Keyword Matching Analysis

DISCUSSION OF KEYWORD MATCHING

Here we calculate a vector of keyword matching counts for each type label from
the storm data file.

``` {r}
## this is the count for a given label keyword
labelKeywordCounts <- sapply(labelKeywords,length)
```

### Labels with no Keyword Matches

``` {r}
## labels with no keyword matches
names(labelKeywords[labelKeywordCounts == 0])
```

### Labels with One Keyword Match

``` {r}
## labels with 1 keyword match
names(labelKeywords[labelKeywordCounts == 1])
```

### Labels with Two Keyword Matches

``` {r}
## labels with 2 keyword matches
names(labelKeywords[labelKeywordCounts == 2])
```

### Labels with Three or More Keyword Matches

``` {r}
## labels with 3 or more keyword matches
names(labelKeywords[labelKeywordCounts >= 3])
```

## Data Files

NOTES ABOUT THE DATA FILES

### NWS Storm Types

TALK ABOUT THIS

Here is a list of the official storm types.

``` {r}

##NWS types, as listed from NWS
print(officialTypes)

```

Here is a function that can be used to generate the storm type file and
save it in the location used by this document.

``` {r}
## This function creates the official types data file and places it in
## the location used by this analysis.
generateStormTypeFile <- function() {
  typeJson <- '["Tsunami", "Storm Surge/Tide", "High Surf", "Seiche", 
                "Rip Current", "Astronomical Low Tide", "Flash Flood", "Flood", 
                "Coastal Flood", "Lakeshore Flood", "Heavy Rain", "Hail", 
                "Lightning", "Hurricane (Typhoon)", "Tropical Depression", 
                "Tropical Storm", "Thunderstorm Wind", "High Wind", "Strong Wind", 
                "Dust Storm", "Tornado", "Funnel Cloud", "Waterspout", 
                "Dust Devil", "Blizzard", "Winter Storm", "Heavy Snow", 
                "Ice Storm", "Lake-Effect Snow", "Sleet", "Winter Weather", 
                "Avalanche", "Extreme Cold/Wind Chill", "Frost/Freeze", 
                "Cold/Wind Chill", "Excessive Heat", "Heat", "Dense Fog", 
                "Freezing Fog", "Debris Flow", "Wildfire", "Dense Smoke", 
                "Drought", "Volcanic Ash", "Marine Hail", "Marine High Wind", 
                "Marine Strong Wind", "Marine Thunderstorm Wind"]'
  
  writeLines(typeJson,"officialTypes.json")
}
```

### Storm Type Aliases

TALK ABOUT THIS

Here is a list of the storm type aliases used in this analysis.

``` {r}
print(typeAliases)
```

Here is a function that can be used to generate the type aliases file and
save it in the location used by this document.

``` {r}
## This function creates the type alias data file and places it in
## the location used by this analysis.
generateTypeAliasFile <- function() {
  typeAliasJson <- '{
  "cold/wind chill": ["cold", "wind chill", "windchill", "hypothermia/exposure", "hyperthermia/exposure"],
  "debris flow": ["landslide", "rock slide", "mud slide", "mudslide", "landslump"],
  "dense fog": ["fog"],
  "dust storm": ["blowing dust"],
  "extreme cold/wind chill": ["extreme cold", "extreme wind chill", "extreme windchill"],
  "freezing fog": ["glaze"],
  "frost/freeze": ["frost", "freeze", "black ice"],
  "flood": ["dam break"],
  "flash flood": ["flash flood/flood", "flood/flash/flood"],
  "heat": ["unseasonably warm", "warm weather"],
  "heavy rain": ["torrential rainfall", "urban/sml stream fld", "hvy rain", "rain", "unseasonal rain"],
  "heavy snow": ["excessive snow"],
  "high surf": ["heavy surf", "rough surf", "hazardous surf"],
  "high wind": ["non tstm wind", "non-tstm wind"],
  "hurricane (typhoon)": ["hurricane", "typhoon", "hurricane/typhoon"],
  "lake-effect snow": ["lake effect snow"],
  "storm surge/tide": ["storm surge"],
  "strong wind": ["gusty wind", "gradient wind", "wind damage", "wind"],
  "thunderstorm wind": ["tstm wind", "tstmw", "gustnado", "downburst", "microburst"],
  "tornado": ["landspout"],
  "wildfire": ["wild fire", "grass fire", "forest fire", "brush fire"],
  "winter weather": ["freezing rain", "light snowfall", "snow and ice", "late season snow", "light snow", "snow", "freezing drizzle", "rain/snow", "snow squall", "mixed precipitation", "blowing snow", "wintry mix", "mixed precip", "snow squalls", "falling snow/ice", "cold and snow", "ice roads", "icy roads", "ice on roads"]
}'
  writeLines(typeAliasJson,"typeAliases.json")
}
```